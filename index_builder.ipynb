{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PDF file and return full text\n",
    "def read_pdf(filepath):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(filepath) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text with RecursiveCharacterTextSplitter\n",
    "def chunk_text(text, chunk_size=600, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save chunks as JSON to chunks/ directory\n",
    "def save_chunks_to_json(chunks, filename, output_dir=\"chunks\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Wrap each chunk as a dict for easier future use\n",
    "    data = [{\"id\": i, \"text\": chunk} for i, chunk in enumerate(chunks)]\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(data)} chunks to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Processing: data/catan_knights_3to4p.pdf\n",
      "âœ… Saved 140 chunks to chunks/catan_knights_3to4p.json\n",
      "ðŸ“š Processing: data/catan_barbarians_3to4p.pdf\n",
      "âœ… Saved 167 chunks to chunks/catan_barbarians_3to4p.json\n",
      "ðŸ“š Processing: data/catan_seafarers_3to4p.pdf\n",
      "âœ… Saved 126 chunks to chunks/catan_seafarers_3to4p.json\n",
      "ðŸ“š Processing: data/catan_knights_5to6p.pdf\n",
      "âœ… Saved 12 chunks to chunks/catan_knights_5to6p.json\n",
      "ðŸ“š Processing: data/catan_barbarians_5to6p.pdf\n",
      "âœ… Saved 30 chunks to chunks/catan_barbarians_5to6p.json\n",
      "ðŸ“š Processing: data/catan_pirates_3to4p.pdf\n",
      "âœ… Saved 112 chunks to chunks/catan_pirates_3to4p.json\n",
      "ðŸ“š Processing: data/catan_pirates_5to4p.pdf\n",
      "âœ… Saved 23 chunks to chunks/catan_pirates_5to4p.json\n",
      "ðŸ“š Processing: data/catan_base_5to6p.pdf\n",
      "âœ… Saved 22 chunks to chunks/catan_base_5to6p.json\n",
      "ðŸ“š Processing: data/catan_base_3to4p.pdf\n",
      "âœ… Saved 111 chunks to chunks/catan_base_3to4p.json\n",
      "ðŸ“š Processing: data/catan_seafarers_5to6p.pdf\n",
      "âœ… Saved 41 chunks to chunks/catan_seafarers_5to6p.json\n"
     ]
    }
   ],
   "source": [
    "# Process all PDFs in /data\n",
    "data_dir = \"data\"\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(data_dir, file)\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        text = read_pdf(pdf_path)\n",
    "        chunks = chunk_text(text, chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "        base_filename = os.path.splitext(file)[0] + \".json\"\n",
    "        save_chunks_to_json(chunks, base_filename)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
