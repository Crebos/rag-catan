{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read PDFs and Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PDF file and return full text\n",
    "def read_pdf(filepath):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(filepath) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text with RecursiveCharacterTextSplitter\n",
    "def chunk_text(text, chunk_size=600, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save chunks as JSON to chunks/ directory\n",
    "def save_chunks_to_json(chunks, filename, output_dir=\"chunks\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Wrap each chunk as a dict for easier future use\n",
    "    data = [{\"id\": i, \"text\": chunk} for i, chunk in enumerate(chunks)]\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(data)} chunks to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Processing: data/catan_knights_3to4p.pdf\n",
      "âœ… Saved 140 chunks to chunks/catan_knights_3to4p.json\n",
      "ðŸ“š Processing: data/catan_barbarians_3to4p.pdf\n",
      "âœ… Saved 167 chunks to chunks/catan_barbarians_3to4p.json\n",
      "ðŸ“š Processing: data/catan_seafarers_3to4p.pdf\n",
      "âœ… Saved 126 chunks to chunks/catan_seafarers_3to4p.json\n",
      "ðŸ“š Processing: data/catan_knights_5to6p.pdf\n",
      "âœ… Saved 12 chunks to chunks/catan_knights_5to6p.json\n",
      "ðŸ“š Processing: data/catan_barbarians_5to6p.pdf\n",
      "âœ… Saved 30 chunks to chunks/catan_barbarians_5to6p.json\n",
      "ðŸ“š Processing: data/catan_pirates_3to4p.pdf\n",
      "âœ… Saved 112 chunks to chunks/catan_pirates_3to4p.json\n",
      "ðŸ“š Processing: data/catan_pirates_5to4p.pdf\n",
      "âœ… Saved 23 chunks to chunks/catan_pirates_5to4p.json\n",
      "ðŸ“š Processing: data/catan_base_5to6p.pdf\n",
      "âœ… Saved 22 chunks to chunks/catan_base_5to6p.json\n",
      "ðŸ“š Processing: data/catan_base_3to4p.pdf\n",
      "âœ… Saved 111 chunks to chunks/catan_base_3to4p.json\n",
      "ðŸ“š Processing: data/catan_seafarers_5to6p.pdf\n",
      "âœ… Saved 41 chunks to chunks/catan_seafarers_5to6p.json\n"
     ]
    }
   ],
   "source": [
    "# Process all PDFs in /data\n",
    "data_dir = \"data\"\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(data_dir, file)\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        text = read_pdf(pdf_path)\n",
    "        chunks = chunk_text(text, chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "        base_filename = os.path.splitext(file)[0] + \".json\"\n",
    "        save_chunks_to_json(chunks, base_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_from_folder(folder_path):\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                all_texts.extend([item['text'] for item in data])\n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_texts(texts, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", chunk_size=256, chunk_overlap=20):\n",
    "    splitter = SentenceTransformersTokenTextSplitter(\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        tokens_per_chunk=chunk_size,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    split_chunks = []\n",
    "    for text in texts:\n",
    "        split_chunks.extend(splitter.split_text(text))\n",
    "    return split_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_index(chunks, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=32):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    \n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 784 documents.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ead636206ea430faae73a0f616a02df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f895d47bf043e49e40d51f2a896943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99601df14684438782b304f5e79988c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06abfcae59c4d189673931c09f927ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2a8883673a4fadb4b6fb4716f3854c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d2402c691047abb2ea253b0d4a7758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8a76f8033a4ded881369342f6e00e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95dcd5ddba9246ad90474618a445e608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89400ed1134045328ea5c8cb12912020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0593cf5970145d8a890b61ab5c9b64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05da59b28c35469b825180e139a46aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 824 token-aware chunks.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288a3e5e890846f9a4b19c6658770d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built.\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"chunks/\"\n",
    "texts = load_chunks_from_folder(folder_path)\n",
    "print(f\"Loaded {len(texts)} documents.\")\n",
    "\n",
    "split_chunks = split_texts(texts)\n",
    "print(f\"Split into {len(split_chunks)} token-aware chunks.\")\n",
    "\n",
    "index, chunk_lookup = embed_and_index(split_chunks)\n",
    "print(\"FAISS index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"index/my_index.faiss\")\n",
    "with open(\"index/chunk_lookup.json\", \"w\") as f:\n",
    "    json.dump(chunk_lookup, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, k=3):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    print(f\"\\nTop {k} results for: \\\"{query}\\\"\\n\")\n",
    "    for rank, (idx, dist) in enumerate(zip(indices[0], distances[0]), start=1):\n",
    "        print(f\"Result #{rank} (Distance: {dist:.4f})\")\n",
    "        print(chunk_lookup[idx])\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 results for: \"I have all the necessary resources and it is my turn. Where can I place a city? Anywhere connected to a road, right?\"\n",
      "\n",
      "Result #1 (Distance: 0.9690)\n",
      "production, you do not receive any resources or player â€™ s metropolis away ), you may place another metropolis commodities, you may take any one resource of your choice gate on one of your cities. but, you may not purchase any from the bank. you many not, however, use this ability when improvements beyond the third level of a given color unless you a â€œ 7 â€ is rolled. have a city where you could build a metropolis. if you do not have such a city on the board, you must wait until you have built at least one more city. illustration l 3 city improvements 88\n",
      "--------------------------------------------------------------------------------\n",
      "Result #2 (Distance: 1.0133)\n",
      "player places 1 city and 1 road. ( the first player to â€“ build roads, settlements, cities, place a settlement will be the last to place a city ). beginners should set up knights, city walls, and / or city no city may be placed fewer than 2 hex edges away the game according to improvements. from any city or settlement. the example shown here. â€“ activate, promote, and / or perform â€¢ each player receives their first actions with knights. resources when they place their city â€” â€“ play any number of progress cards. receiving 1 resource for each terrain adjacent to their city. â€¢ players place their\n",
      "--------------------------------------------------------------------------------\n",
      "Result #3 (Distance: 1.0321)\n",
      "own roads. regardless of whose turn it is ( i. e., during any production phase ), when a terrain hex produces resources, you b ) playing development cards y receive 1 resource card for each settlement you have adjacent at any time during your turn, you may play 1 development to that terrain hex. card ( put it face up on the table ). that card, however, may not each settlement is worth 1 victory point. be a card you bought during the same turn ( except for a victory c ) city y requires : 3 ore & 2 grain point card, as described below )! you may only establish a city by knight cards ( purple frame ) y\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "query = \"I have all the necessary resources and it is my turn. Where can I place a city? Anywhere connected to a road, right?\"\n",
    "search_faiss(query, k=3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
