{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read PDFs and Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PDF file and return full text\n",
    "def read_pdf(filepath):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(filepath) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_smart(filepath): \n",
    "    full_text = \"\"\n",
    "    with pdfplumber.open(filepath) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text(x_tolerance=1.5, y_tolerance=1.5, layout=True)\n",
    "            if text:\n",
    "                full_text += text + \"\\n\"\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk text with RecursiveCharacterTextSplitter\n",
    "def chunk_text(text, chunk_size=600, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_smart(text):\n",
    "    lines = text.splitlines()\n",
    "    sections = []\n",
    "    current_heading = \"\"\n",
    "    buffer = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped = line.strip()\n",
    "        if is_heading(stripped):\n",
    "            # Save previous section\n",
    "            if buffer:\n",
    "                sections.append(current_heading + \"Content: \" + \"\\n\".join(buffer).strip())\n",
    "                buffer = []\n",
    "            current_heading = stripped\n",
    "        else:\n",
    "            buffer.append(stripped)\n",
    "\n",
    "    return sections\n",
    "\n",
    "def is_heading(line):\n",
    "    line = line.strip()\n",
    "    # Good heuristics for heading lines:\n",
    "    return (\n",
    "        len(line) < 100 and (\n",
    "            re.match(r\"^[A-Z][A-Z\\s]{3,}$\", line) or  # ALL CAPS\n",
    "            re.match(r\"^[A-Z][a-z]{1,15}(\\s+[A-Z][a-z]{1,15}){0,4}$\", line) or  # Title Case\n",
    "            re.match(r\"^[a-z]?\\)?\\s?[A-Z].{0,50}$\", line)  # a) Heading Y style\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save chunks as JSON to chunks/ directory\n",
    "def save_chunks_to_json(chunks, filename, output_dir=\"chunks\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Wrap each chunk as a dict for easier future use\n",
    "    data = [{\"id\": i, \"text\": chunk} for i, chunk in enumerate(chunks)]\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(data)} chunks to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š Processing: data/catan_knights_3to4p.pdf\n",
      "âœ… Saved 140 chunks to chunks/catan_knights_3to4p.json\n",
      "ðŸ“š Processing: data/catan_barbarians_3to4p.pdf\n",
      "âœ… Saved 167 chunks to chunks/catan_barbarians_3to4p.json\n",
      "ðŸ“š Processing: data/catan_seafarers_3to4p.pdf\n",
      "âœ… Saved 126 chunks to chunks/catan_seafarers_3to4p.json\n",
      "ðŸ“š Processing: data/catan_knights_5to6p.pdf\n",
      "âœ… Saved 12 chunks to chunks/catan_knights_5to6p.json\n",
      "ðŸ“š Processing: data/catan_barbarians_5to6p.pdf\n",
      "âœ… Saved 30 chunks to chunks/catan_barbarians_5to6p.json\n",
      "ðŸ“š Processing: data/catan_pirates_3to4p.pdf\n",
      "âœ… Saved 112 chunks to chunks/catan_pirates_3to4p.json\n",
      "ðŸ“š Processing: data/catan_pirates_5to4p.pdf\n",
      "âœ… Saved 23 chunks to chunks/catan_pirates_5to4p.json\n",
      "ðŸ“š Processing: data/catan_base_5to6p.pdf\n",
      "âœ… Saved 22 chunks to chunks/catan_base_5to6p.json\n",
      "ðŸ“š Processing: data/catan_base_3to4p.pdf\n",
      "âœ… Saved 111 chunks to chunks/catan_base_3to4p.json\n",
      "ðŸ“š Processing: data/catan_seafarers_5to6p.pdf\n",
      "âœ… Saved 41 chunks to chunks/catan_seafarers_5to6p.json\n"
     ]
    }
   ],
   "source": [
    "# Process all PDFs in /data\n",
    "data_dir = \"data\"\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(data_dir, file)\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        text = read_pdf(pdf_path)\n",
    "        chunks = chunk_text(text, chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "        base_filename = os.path.splitext(file)[0] + \".json\"\n",
    "        save_chunks_to_json(chunks, base_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: data/catan_knights_3to4p.pdf\n",
      "âœ… Saved 296 chunks to chunks/catan_knights_3to4p.json\n",
      "Processing: data/catan_barbarians_3to4p.pdf\n",
      "âœ… Saved 254 chunks to chunks/catan_barbarians_3to4p.json\n",
      "Processing: data/catan_seafarers_3to4p.pdf\n",
      "âœ… Saved 228 chunks to chunks/catan_seafarers_3to4p.json\n",
      "Processing: data/catan_knights_5to6p.pdf\n",
      "âœ… Saved 25 chunks to chunks/catan_knights_5to6p.json\n",
      "Processing: data/catan_barbarians_5to6p.pdf\n",
      "âœ… Saved 74 chunks to chunks/catan_barbarians_5to6p.json\n",
      "Processing: data/catan_pirates_3to4p.pdf\n",
      "âœ… Saved 209 chunks to chunks/catan_pirates_3to4p.json\n",
      "Processing: data/catan_pirates_5to4p.pdf\n",
      "âœ… Saved 25 chunks to chunks/catan_pirates_5to4p.json\n",
      "Processing: data/catan_base_5to6p.pdf\n",
      "âœ… Saved 46 chunks to chunks/catan_base_5to6p.json\n",
      "Processing: data/catan_base_3to4p.pdf\n",
      "âœ… Saved 238 chunks to chunks/catan_base_3to4p.json\n",
      "Processing: data/catan_seafarers_5to6p.pdf\n",
      "âœ… Saved 53 chunks to chunks/catan_seafarers_5to6p.json\n"
     ]
    }
   ],
   "source": [
    "# Process all PDFs smartly in /data\n",
    "data_dir = \"data\"\n",
    "for file in os.listdir(data_dir):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(data_dir, file)\n",
    "        print(f\"Processing: {pdf_path}\")\n",
    "        text = read_pdf_smart(pdf_path)\n",
    "        chunks = chunk_text_smart(text)\n",
    "\n",
    "        base_filename = os.path.splitext(file)[0] + \".json\"\n",
    "        save_chunks_to_json(chunks, base_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize and Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_from_folder(folder_path):\n",
    "    all_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                all_texts.extend([item['text'] for item in data])\n",
    "    return all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_texts(texts, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", chunk_size=256, chunk_overlap=20):\n",
    "    splitter = SentenceTransformersTokenTextSplitter(\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        tokens_per_chunk=chunk_size,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    split_chunks = []\n",
    "    for text in texts:\n",
    "        split_chunks.extend(splitter.split_text(text))\n",
    "    return split_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_index(chunks, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", batch_size=32):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(chunks, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)\n",
    "    \n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1448 documents.\n",
      "Split into 1542 token-aware chunks.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda470fbf3514221baafd9543d416f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index built.\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"chunks/\"\n",
    "texts = load_chunks_from_folder(folder_path)\n",
    "print(f\"Loaded {len(texts)} documents.\")\n",
    "\n",
    "split_chunks = split_texts(texts)\n",
    "print(f\"Split into {len(split_chunks)} token-aware chunks.\")\n",
    "\n",
    "index, chunk_lookup = embed_and_index(split_chunks)\n",
    "print(\"FAISS index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, \"index/my_index.faiss\")\n",
    "with open(\"index/chunk_lookup.json\", \"w\") as f:\n",
    "    json.dump(chunk_lookup, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, k=3):\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    print(f\"\\nTop {k} results for: \\\"{query}\\\"\\n\")\n",
    "    for rank, (idx, dist) in enumerate(zip(indices[0], distances[0]), start=1):\n",
    "        print(f\"Result #{rank} (Distance: {dist:.4f})\")\n",
    "        print(chunk_lookup[idx])\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 results for: \"I have all the necessary resources and it is my turn. Where can I place a city? Anywhere connected to a road, right?\"\n",
      "\n",
      "Result #1 (Distance: 0.9765)\n",
      "you cannot build a city directly. you can onlycontent : to make the sequence easier to learn for beginners. we upgrade an existing settlement to a city. you pay recommend experienced players ignore this separation. the required resources, return the settlement to after rolling for resource production, you can trade and build your supply, and replace the settlement with a city on the same intersection y. each city is worth 2 victory points. you receive in any order ( you can trade, build, trade again and build again, etc. ). you can even use a harbor on the same turn you build a double resource production ( 2 resource cards ) from the settlement there. using this method speeds up the game a lot. adjacent terrain hexes whenever those numbers are rolled. 66\n",
      "--------------------------------------------------------------------------------\n",
      "Result #2 (Distance: 1.0221)\n",
      "almanac under â€œ combined trade / build phase y. â€ content : settlements, or cities. only 1 road can be\n",
      "--------------------------------------------------------------------------------\n",
      "Result #3 (Distance: 1.0345)\n",
      "one city improvementcontent : ( abbey, town hall, etc. ) that you build this turn costs one less commodity than usual.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "query = \"I have all the necessary resources and it is my turn. Where can I place a city? Anywhere connected to a road, right?\"\n",
    "search_faiss(query, k=3)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
